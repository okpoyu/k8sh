# run reloadExtensions to reload

# remove alias
rmalias() {
  if alias $1 2>/dev/null; then
    unalias $1
  fi
}

# remove function
rmfunction() {
  if declare -f -F $1 > /dev/null; then
    unset -f $1
  fi
}

# add highlighting for substrings
highlight() {
    declare -A fg_color_map
    fg_color_map[black]=30
    fg_color_map[red]=31
    fg_color_map[green]=32
    fg_color_map[yellow]=33
    fg_color_map[blue]=34
    fg_color_map[magenta]=35
    fg_color_map[cyan]=36

    fg_c=$(echo -e "\e[1;${fg_color_map[$1]}m")
    c_rs=$'\e[0m'
    sed -u s"/$2/$fg_c\0$c_rs/g"
}

bashwatch() {
    while true; do clear; $@; sleep 2; done
}

alias watch="bashwatch"

# Displays all nodes in the cluster including kubernetes version info
alias nodes="k get nodes -o=custom-columns=Name:.metadata.name,IP:.status.addresses[0].address,Kubelet:.status.nodeInfo.kubeletVersion,Health:.status.conditions[*].reason,CPU:.status.allocatable.cpu,MEM:.status.allocatable.memory | highlight green 'KubeletReady' | highlight green 'KernelHasNoDeadlock' | highlight green 'KubeletHasNoDiskPressure' | highlight green 'KubeletHasSufficientMemory' | highlight green 'KubeletHasSufficientDisk'"

# Force a deletion
alias deletef="k delete --grace-period=0 --force"

# Get events in order
alias events-failed="k get events --sort-by='{lastTimestamp}' --field-selector type!=Normal"

#Get all failed events in all namespaces
alias events-failed-all="k get events --sort-by='{lastTimestamp}' --field-selector type!=Normal --all-namespaces"

# get images that pods use in your namespace with a count
alias pod-images="k get pods --no-headers -o=custom-columns=Images:.spec.containers[*].image | tr ',' '\n' | sort | uniq -c"

# get pods and their images sorted by image
alias pods-with-image="k get pods --no-headers -o=go-template --template='{{range .items}}{{\$pod := .metadata.name}}{{range .spec.containers}}{{\$pod}}{{\"\t\"}}{{.name}}{{\"\t\"}}{{.image }}{{\"\n\"}}{{end}}{{end}}' | sort -k 3"

# get overall pod status report
alias pod-report="k get pods --all-namespaces --no-headers -o=custom-columns=Status:.status.phase | sort | uniq -c | highlight green Running"

# get pods that are current leaders
alias leaders="k get ep -o json | jq '.items[].metadata.annotations[\"control-plane.alpha.kubernetes.io/leader\"] | select(.!=null)' -r | jq '.holderIdentity' -r"

# pods with colours
rmalias pods
pods() {
  k get pods $@ | highlight green 'Running'
}

all-pods() {
  k get pods --all-namespaces $@ | highlight green 'Running'
}

all-pods-on-node() {
  local NODE_NAME=$1
  shift
  k get pods -o wide --all-namespaces \
             --field-selector spec.nodeName=${NODE_NAME} $@
}

all-failed-pods() {
  k get pods --all-namespaces --field-selector status.phase!=Running --sort-by={.spec.nodeName} $@ | highlight green 'Running'
}


all-failed-pods-on-node() {
  local NODE_NAME=$1
  shift
  k get pods -o wide --all-namespaces \
             --field-selector status.phase!=Running,spec.nodeName=${NODE_NAME} $@

}

ssh-nodes() {
  SSH_ARGS="${@}"
  k get nodes --no-headers -o=custom-columns=IP:.status.addresses[0].address | \
    xargs -INODE -n1 -P1 sh -c "ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no ${K8S_SSH_USER-core}@NODE ${SSH_ARGS} | sed -e 's/^/NODE:\t/'"
}

nodeips() {
  k get nodes -o jsonpath={.items[*].status.addresses[?\(@.type==\"ExternalIP\"\)].address}
}

namesof() {
  local type=${1:-'pods'}
  k get --no-headers $type | awk '{ print $1 }' | tr '\n' ',' | sed 's/,$/\n/'
}

namesofgrep() {
  local search=${1:-'-'}
  local type=${2:-'pods'}
  k get --no-headers -o wide $type | grep $search | awk '{ print $1 }' | tr '\n' ',' | sed 's/,$/\n/'
}

# output cert as text
readcert() {
  k get secrets -o json $@ | jq '.data["tls.crt"]' -r | base64 -d | openssl x509 -in /dev/stdin -text -noout
}

# add grep to pods/nodes listing, eg:
# $ pods? -v Running
# $ nodes? cr2
# $ pods-with-image? hyperkube
alias pods?="pods -o wide | grep"
alias all-pods?="all-pods -o wide | grep"
alias nodes?="nodes | grep"
alias pod-images?="pod-images | grep"
alias pods-with-image?="pods-with-image | grep"
alias leaders?="leaders | grep"

# Pick a random node in Ready state
get-random-node() {
  k get nodes | grep -v etcd | grep ' Ready' | awk '{ print $1 }' | shuf -n 1
}

# Show pods that don't have all containers ready
alias brokenpods='k get pod --all-namespaces | grep -vE '\''([0-9]+)/\1'\'''
alias brokenpodswide='k get pod --all-namespaces -o wide | grep -vE '\''([0-9]+)/\1'\'''

# Runs a busybox container (optionally selecting the a specific node to run on)
# and shells into it.
toolbox() {
  delete po/toolbox --ignore-not-found=true
  NODETOUSE="$(get-random-node)"
  SERVICE_ACCOUNT="default"
  INJECT_SIDECAR="false"
  while [ "$1" = "--node" -o "$1" = "--sa" -o "$1" = "--inject-sidecar" ]; do
    if [ "$1" = "--node" ] ; then
        NODETOUSE="$2"
        shift 2
    elif [ "$1" = "--sa" ] ; then
        SERVICE_ACCOUNT="$2"
        shift 2
    elif [ "$1" = "--inject-sidecar" ] ; then
        INJECT_SIDECAR="true"
        shift
    fi
  done
  OVERRIDE_PARAM="{ \"apiVersion\": \"v1\", \"spec\": { \"nodeName\": \"$NODETOUSE\"}, \"metadata\": {\"annotations\": { \"sidecar.istio.io/inject\":\"$INJECT_SIDECAR\" } } }"
  echo "Restricting pod to node: $NODETOUSE using service account: $SERVICE_ACCOUNT"
  k run -i --tty toolbox --limits='cpu=200m,memory=512Mi' --image=mirror-releases.docker.ospcfc.tech.lastmile.com/osp-cfc-platform/toolbox:0.1.0 --restart=Never --serviceaccount="$SERVICE_ACCOUNT" --overrides="$OVERRIDE_PARAM" -- bash
}

# Runs a temporary kube-scheduler
# Assumes self hosting setup
temp-scheduler() {
  delete pod/temp-scheduler
  U=""
  NODETOUSE="$(get-random-node)"
  if [[ $1 ]]; then
    NODETOUSE=$1
  fi
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  k $U --namespace kube-system get deployment kube-scheduler -o json | jq '.spec.template * {"kind": "Pod", "apiVersion": "v1", "metadata": {"name": "temp-scheduler", "namespace": "kube-system", "labels": {"k8s-app": "temp-scheduler"}}}' | jq ".spec.nodeName = \"$NODETOUSE\"" | kubectl $U create -f -
}

# Runs a temporary kube-controller-manager
# Assumes self hosting setup
temp-controller-manager() {
  delete pod/temp-controller-manager
  U=""
  NODETOUSE="$(get-random-node)"
  if [[ $1 ]]; then
    NODETOUSE=$1
  fi
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  k $U --namespace kube-system get deployment kube-controller-manager -o json | jq '.spec.template * {"kind": "Pod", "apiVersion": "v1", "metadata": {"name": "temp-controller-manager", "namespace": "kube-system", "labels": {"k8s-app": "temp-controller-manager"}}}' | jq ".spec.nodeName = \"$NODETOUSE\"" | kubectl $U create -f -
}

# removes the temporary scheduler and controller manager when not required
delete-temp-components() {
  U=""
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  delete $U pod/temp-scheduler
  delete $U pod/temp-controller-manager
}

# delete pods with status
delete-pods-with-status() {
    local POD_STATUS=$1
    pod_list=($(brokenpods | grep ${POD_STATUS} | awk '{print $1" "$2}'))
    x=0
    while [[ ${x} -lt ${#pod_list[*]} ]]
    do
        k delete po ${pod_list[$x+1]} -n ${pod_list[$x]} --wait=false
        x=$(( $x + 2 ))
    done
}

alias istioctl="istioctl --context \$KUBECTL_CONTEXT --namespace \$KUBECTL_NAMESPACE"

node-shell() {
  local NODE="$1"; shift
  local COMMAND="$@"
  local IMAGE="mirror-hub.docker.tech.lastmile.com/library/alpine"
  local POD="nsenter-$(env LC_ALL=C tr -dc a-z0-9 < /dev/urandom | head -c 6)"
  if [ -z "${COMMAND}" ]; then
    COMMAND="bash -l"
  fi
  if [ -z "${NODE}" ]; then
    echo "usage: node-shell <nodename>" 1>&2
    return
  fi
  local OVERRIDES="$(
cat <<EOT
{
  "spec": {
    "nodeName": "$NODE",
    "hostPID": true,
    "containers": [
      {
        "securityContext": {
          "privileged": true
        },
        "image": "$IMAGE",
        "name": "nsenter",
        "stdin": true,
        "stdinOnce": true,
        "tty": true,
        "command": [ "nsenter", "--target", "1", "--mount", "--uts", "--ipc", "--net", "--pid", "--", "bash", "-c", "${COMMAND}" ]
      }
    ]
  }
}
EOT
  )"
  echo "${OVERRIDES}"
  echo "spawning \"$POD\" on \"$NODE\""
  k run -n default --rm --image "$IMAGE" --overrides="$OVERRIDES" --generator=run-pod/v1 -ti "$POD"
}

check_certs() {
  local SECRET=$1; shift
  local GET_ARGS="$@"
  local RETURN=0
  CERTS=$(
    for PEM in $(get secret ${GET_ARGS} ${SECRET} -o json \
      | jq -r '.data[]'); do
        echo ${PEM} \
        | base64 -d
      done \
      | xargs \
      | sed -e 's/ //g' -e 's/-----[^-]*-----/ /g')
  for CERT in ${CERTS}; do
    local CERT=$(echo -e "-----BEGIN CERTIFICATE-----\n${CERT}\n-----END CERTIFICATE-----")
    if echo "${CERT}" | openssl x509 -noout 2>/dev/null; then
      echo "${CERT}" | openssl x509 -noout -checkend 1 -subject -enddate
      ((RETURN+=$?))
    fi
  done
  return ${RETURN}
}
istio_secret() {
  local POD=$1
  k get pod ${POD} -o json \
    | jq -r '.spec.volumes[]
    | select(.name == "istio-certs")
    | .secret.secretName'
}
invalid_istio_secrets() {
  while read LINE; do
    set ${LINE}
    if ! check_certs $2 -n $1 > /dev/null; then
      echo "${LINE}"
    fi
  done < <(get secret --field-selector type=istio.io/key-and-cert --all-namespaces --no-headers)
}
delete_invalid_istio_secrets() {
  local SLEEP=$1
  if [ -z "${SLEEP}" ]; then
    SLEEP=60
  fi
  while read LINE; do
    set ${LINE}
#    if echo ${LINE} | grep istio\.system >/dev/null; then continue; fi
    echo delete secret -n $1 $2
    delete secret -n $1 $2 --wait=false
    echo "Waiting ${SLEEP} seconds"; sleep ${SLEEP}
  done < <(invalid_istio_secrets)
}

# Runs a temporary kube-scheduler
# Assumes self hosting setup
temp-scheduler() {
  delete pod/temp-scheduler
  U=""
  NODETOUSE="$(get-random-node)"
  if [[ $1 ]]; then
    NODETOUSE=$1
  fi
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  k $U --namespace kube-system get deployment kube-scheduler -o json | jq '.spec.template * {"kind": "Pod", "apiVersion": "v1", "metadata": {"name": "temp-scheduler", "namespace": "kube-system", "labels": {"k8s-app": "temp-scheduler"}}}' | jq ".spec.nodeName = \"$NODETOUSE\"" | kubectl $U create -f -
}

# Runs a temporary kube-controller-manager
# Assumes self hosting setup
temp-controller-manager() {
  delete pod/temp-controller-manager
  U=""
  NODETOUSE="$(get-random-node)"
  if [[ $1 ]]; then
    NODETOUSE=$1
  fi
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  k $U --namespace kube-system get deployment kube-controller-manager -o json | jq '.spec.template * {"kind": "Pod", "apiVersion": "v1", "metadata": {"name": "temp-controller-manager", "namespace": "kube-system", "labels": {"k8s-app": "temp-controller-manager"}}}' | jq ".spec.nodeName = \"$NODETOUSE\"" | kubectl $U create -f -
}

# removes the temporary scheduler and controller manager when not required
delete-temp-components() {
  U=""
  if [[ $KUSER ]]; then
    U="--user=$KUSER"
  fi
  delete $U pod/temp-scheduler
  delete $U pod/temp-controller-manager
}

pods_with_invalid_istio_certs() {
  while read LINE; do
    set ${LINE}
    echo -e "\n\e[31m+++  Certs for pod $1  +++\e[0m"
    istioctl pc s $1
  done < <(k get po -o name | cut -d '/' -f 2)
}

stern() {
  /home/okpoyu/go/bin/stern --context ${KUBECTL_CONTEXT} -n ${KUBECTL_NAMESPACE} "$@"
}

access-matrix() {
  /home/okpoyu/.krew/bin/kubectl-access_matrix --context ${KUBECTL_CONTEXT} -n ${KUBECTL_NAMESPACE} "$@"
}

krew() {
  /home/okpoyu/.krew/bin/kubectl-krew "$@"
}

telepresence() {
  /usr/bin/telepresence --context ${KUBECTL_CONTEXT} --namespace ${KUBECTL_NAMESPACE} "$@"
}
